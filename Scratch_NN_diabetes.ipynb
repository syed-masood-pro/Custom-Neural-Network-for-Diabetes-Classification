{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "zsUOb70Leufr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Dense:\n",
        "    def __init__(self,n_inputs,n_neurons,weight_regularizer_l1=0,weight_regularizer_l2=0,bias_regularizer_l1=0,bias_regularizer_l2=0):\n",
        "        self.weights=0.01*np.random.randn(n_inputs,n_neurons) # Initialize weights with small random values (scaled by 0.01) for better convergence\n",
        "        self.biases=np.zeros((1,n_neurons)) # Initialize biases to zeros\n",
        "        # Set regularization strengths for weights (L1 and L2)\n",
        "        self.weight_regularizer_l1=weight_regularizer_l1\n",
        "        self.weight_regularizer_l2=weight_regularizer_l2\n",
        "        # Set regularization strengths for biases (L1 and L2)\n",
        "        self.bias_regularizer_l1=bias_regularizer_l1\n",
        "        self.bias_regularizer_l2=bias_regularizer_l2\n",
        "\n",
        "    # compute the layer's output using the formula: output = inputs â€¢ weights + biases\n",
        "    def forward(self, inputs):\n",
        "        self.inputs=inputs\n",
        "        self.output=np.dot(inputs,self.weights)+self.biases\n",
        "\n",
        "    # compute gradients of the loss with respect to inputs, weights, and biases\n",
        "    def backward(self,dvalues):\n",
        "        self.dweights=np.dot(self.inputs.T,dvalues) # Gradient with respect to weights: (inputs^T x dvalues)\n",
        "        self.dbiases=np.sum(dvalues,axis=0,keepdims=True) # Gradient with respect to biases: sum over dvalues\n",
        "\n",
        "        # L1 Weight - calculate only when l1 is greater than 0\n",
        "        if self.weight_regularizer_l1 >0:\n",
        "            dl1=np.ones_like(self.weights) # Create a matrix of ones with the same shape as weights\n",
        "            dl1[self.weights<0]=-1  # For negative weight values, the gradient is -1\n",
        "            self.dweights+=self.weight_regularizer_l1 *dl1  # Add the L1 regularization term to the gradient of the weights\n",
        "\n",
        "        # L2 Weight - calculate only when l2 is greater than 0\n",
        "        if self.weight_regularizer_l2 >0:\n",
        "            self.dweights += 2* self.weight_regularizer_l2*self.weights # Add the L2 regularization term (derivative of squared weights)\n",
        "\n",
        "        # L1 Bias - calculate only when l1 is greater than 0\n",
        "        if self.bias_regularizer_l1 >0:\n",
        "            dl1=np.ones_like(self.biases)\n",
        "            dl1[self.biases<0]=-1\n",
        "            self.dbiases+=self.bias_regularizer_l1 *dl1\n",
        "\n",
        "        # L2 Bias - calculate only when l2 is greater than 0\n",
        "        if self.bias_regularizer_l2 >0:\n",
        "            self.dbiases += 2* self.bias_regularizer_l2*self.biases\n",
        "\n",
        "        # Gradient with respect to inputs\n",
        "        self.dinputs=np.dot(dvalues,self.weights.T) # Compute using the chain rule: (dvalues dot weights^T)"
      ],
      "metadata": {
        "id": "tauZ0Vsneyk-"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReLU:\n",
        "    def forward(self,inputs):\n",
        "        self.inputs=inputs\n",
        "        self.output=np.maximum(0,inputs) # It returns the maximum value between 0 and each element in the inputs\n",
        "\n",
        "    def backward(self,dvalues):\n",
        "        self.dinputs=dvalues.copy()\n",
        "        self.dinputs[self.inputs<=0]=0# Zero out gradients where the input values were less than or equal to 0 because the derivative of ReLU is 0 for negative inputs (and 0 at 0)\n",
        ""
      ],
      "metadata": {
        "id": "IScmrRDPhpt8"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Softmax:\n",
        "    def forward(self,inputs):\n",
        "      # Subtract the maximum value in each row for numerical stability\n",
        "      exp_values=np.exp(inputs-np.max(inputs,axis=1,keepdims=True)) # This prevents very large exponentials that could lead to overflow\n",
        "\n",
        "      # Sum the exponentiated values along each row\n",
        "      prob=exp_values/np.sum(exp_values,axis=1,keepdims=True) # This sum is used to normalize the values so that they add up to 1\n",
        "      self.output=prob"
      ],
      "metadata": {
        "id": "9dvjKyediCMU"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Loss:\n",
        "  def calculate(self,output,y):\n",
        "        sample_losses=self.forward(output,y) # Compute individual sample losses using the forward method (which should be defined in a subclass)\n",
        "        data_loss=np.mean(sample_losses) # Compute the mean of these losses to get the overall data loss\n",
        "        return data_loss\n",
        "\n",
        "  def regularization_loss(self,layer):\n",
        "      regularization_loss=0\n",
        "\n",
        "      # If L1 regularization is applied on weights, add its contribution\n",
        "      if layer.weight_regularizer_l1 >0:\n",
        "          regularization_loss+= layer.weight_regularizer_l1 * np.sum(np.abs(layer.weights)) # L1 regularization is the sum of absolute values of the weights, scaled by the L1 strength\n",
        "\n",
        "      # If L2 regularization is applied on weights, add its contribution\n",
        "      if layer.weight_regularizer_l2 > 0:\n",
        "          regularization_loss+= layer.weight_regularizer_l2 * np.sum(layer.weights * layer.weights) # L2 regularization is the sum of squared weights, scaled by the L2 strength\n",
        "\n",
        "      # If L1 regularization is applied on biases, add its contribution\n",
        "      if layer.bias_regularizer_l1 > 0:\n",
        "          regularization_loss += layer.bias_regularizer_l1 * np.sum(np.abs(layer.biases)) # L1 regularization for biases is the sum of absolute values of the biases\n",
        "\n",
        "      # If L2 regularization is applied on biases, add its contribution\n",
        "      if layer.bias_regularizer_l2 > 0:\n",
        "          regularization_loss += layer.bias_regularizer_l2 * np.sum(layer.biases * layer.biases) # L2 regularization for biases is the sum of squared biases\n",
        "\n",
        "      return regularization_loss\n",
        ""
      ],
      "metadata": {
        "id": "8lGOItdbnUA-"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Loss_CategoricalCrossEntropy(Loss):\n",
        "    def forward(self,y_pred,y_true):\n",
        "        samples=len(y_true)\n",
        "        y_pred_clipped=np.clip(y_pred,1e-7,1-1e-7)# The predictions are constrained between 1e-7 and 1 - 1e-7\n",
        "\n",
        "        # If y_true is a 1D array (sparse labels), get the probability for the correct class for each sample\n",
        "        if len(y_true.shape)==1:\n",
        "            correct_confidence=y_pred_clipped[range(samples),y_true] # Using advanced indexing to select the predicted probability of the true class\n",
        "        elif len(y_true.shape)==2: # If y_true is a 2D array (one-hot encoded labels), compute the confidence by element-wise multiplication\n",
        "            correct_confidence=np.sum(y_pred_clipped*y_true,axis=1)  # Sum across classes to get the predicted probability corresponding to the true class.\n",
        "\n",
        "        negative_log=-np.log(correct_confidence)\n",
        "        return negative_log\n",
        "\n",
        "    def backward(self, dvalues, y_true):\n",
        "        samples = len(dvalues)\n",
        "        labels = len(dvalues[0])\n",
        "\n",
        "        # If y_true is a 1D array (sparse labels), convert it to one-hot encoding\n",
        "        if len(y_true.shape) == 1:\n",
        "            y_true = np.eye(labels)[y_true]\n",
        "\n",
        "        # Calculating the gradient of the loss with respect to the predictions\n",
        "        self.dinputs = -y_true / dvalues\n",
        "        # Normalize the gradient by dividing by the number of samples to average the effect\n",
        "        self.dinputs = self.dinputs / samples"
      ],
      "metadata": {
        "id": "gGyaZBSYoUZb"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Softmax_CrossEntropy:\n",
        "    def __init__(self):\n",
        "        # Creating an instance of the Softmax activation function and categorical cross-entropy loss\n",
        "        self.activation=Softmax()\n",
        "        self.loss=Loss_CategoricalCrossEntropy()\n",
        "\n",
        "    def forward(self,inputs,y_true):\n",
        "        self.activation.forward(inputs) # Apply the softmax activation to the input values\n",
        "        self.output=self.activation.output # Retrieving the computed probabilities\n",
        "        return self.loss.calculate(self.output,y_true)  # Calculate and return the loss using the cross-entropy loss's calculate method, which internally calls its forward method\n",
        "\n",
        "    def backward(self,dvalue,y_true):\n",
        "        samples=len(dvalue)\n",
        "\n",
        "        # If y_true is one-hot encoded (i.e., a 2D array), convert it to class indices\n",
        "        if len(y_true.shape)==2:\n",
        "            y_true=np.argmax(y_true,axis=1)\n",
        "\n",
        "        self.dinputs=dvalue.copy()\n",
        "\n",
        "        self.dinputs[range(samples),y_true]-=1  # For each sample, subtract 1 from the probability corresponding to the true class.This step computes the derivative of the combined softmax and cross-entropy loss\n",
        "        self.dinputs=self.dinputs/samples # Normalize the gradients by dividing by the number of samples, so that the gradient is averaged over the batch"
      ],
      "metadata": {
        "id": "g2-vohlapOu2"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dropout:\n",
        "    def __init__(self,rate):\n",
        "        # The 'rate' parameter is the dropout rate (fraction of neurons to drop), We calculate the \"keep probability\" by subtracting the dropout rate from 1\n",
        "        self.rate=1-rate\n",
        "\n",
        "    def forward(self,inputs):\n",
        "        self.inputs=inputs\n",
        "        #Generate a binary mask with the same shape as 'inputs':\n",
        "        # Each element is drawn from a binomial distribution (0 or 1) with probability 'self.rate', Dividing by self.rate scales the activations so that the expected value remains unchanged\n",
        "        self.binary_mask=np.random.binomial(1,self.rate,size=inputs.shape)/self.rate\n",
        "        self.output=inputs * self.binary_mask # Apply the dropout mask to the inputs (element-wise multiplication)\n",
        "\n",
        "    def backward(self,dvalues):\n",
        "        # During backpropagation, only the neurons that were kept (mask value 1) will receive the gradient, Multiply the incoming gradient (dvalues) by the binary mask to pass the gradient only through active neurons\n",
        "        self.dinputs=dvalues*self.binary_mask"
      ],
      "metadata": {
        "id": "PhExwWMr0Eey"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# momentum opti\n",
        "class SGD:\n",
        "    def __init__(self,learning_rate=1.,decay=0.,momentum=0.):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.current_learning_rate =learning_rate # Initialize the current learning rate (this may change over iterations due to decay)\n",
        "        self.decay=decay\n",
        "        self.iterations = 0  # Initialize the iteration counter\n",
        "        self.momentum =momentum\n",
        "\n",
        "    def pre_update_param(self):\n",
        "        # If decay is set, adjust the current learning rate based on the iteration number\n",
        "        if self.decay:\n",
        "            self.current_learning_rate=self.learning_rate*(1./(1.+self.decay *self.iterations)) # This applies a simple decay formula: lr = initial_lr / (1 + decay * iterations)\n",
        "\n",
        "    def update_param(self,layer):\n",
        "        if self.momentum:\n",
        "            # If momentum is enabled, first check if the layer has momentum arrays\n",
        "            if not hasattr(layer, 'weight_momentums'):\n",
        "                # Initialize momentum arrays for weights and biases as zeros with the same shape as the parameters\n",
        "                layer.weight_momentums=np.zeros_like(layer.weights)\n",
        "                layer.bias_momentums=np.zeros_like(layer.biases)\n",
        "\n",
        "            # Compute the update for weights using momentum:\n",
        "            # new_update = (momentum * previous_momentum) - (current_learning_rate * current_gradient)\n",
        "            weight_updates=self.momentum*layer.weight_momentums - self.current_learning_rate * layer.dweights\n",
        "            layer.weight_momentums=weight_updates\n",
        "\n",
        "            bias_updates=self.momentum*layer.bias_momentums - self.current_learning_rate * layer.dbiases\n",
        "            layer.bias_momentums=bias_updates\n",
        "        else:\n",
        "            # If momentum is not used, compute the update directly as the negative gradient scaled by the learning rate using vanilla SGD\n",
        "            layer.weights+= -self.current_learning_rate * layer.dweights\n",
        "            layer.biases+= -self.current_learning_rate * layer.dbiases\n",
        "\n",
        "        # Update the layer's weights and biases by adding the computed updates\n",
        "        layer.weights+=weight_updates\n",
        "        layer.biases+=bias_updates\n",
        "\n",
        "    def post_update_param(self):\n",
        "        self.iterations+=1 # After updating parameters, increment the iteration counter\n",
        "\n"
      ],
      "metadata": {
        "id": "Uq2N4MSWqdID"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Adagrad:\n",
        "    def __init__(self,learning_rate=1.,decay=0.,epsilon=1e-7):\n",
        "        self.learning_rate =learning_rate\n",
        "        self.current_learning_rate=learning_rate # Initialize the current learning rate (this may change over iterations due to decay)\n",
        "        self.decay=decay\n",
        "        self.iterations=0 # Initialize the iteration counter\n",
        "        self.epsilon=epsilon\n",
        "\n",
        "    def pre_update_param(self):\n",
        "        # If decay is set, adjust the current learning rate based on the iteration number\n",
        "        if self.decay:\n",
        "            self.current_learning_rate=self.learning_rate*(1./(1.+self.decay *self.iterations)) # This applies a simple decay formula: lr = initial_lr / (1 + decay * iterations)\n",
        "\n",
        "    def update_param(self,layer):\n",
        "        # Check if the layer already has a cache for weights and biases; if not, initialize them\n",
        "        if not  hasattr(layer,'weight_cache'):\n",
        "            # Initialize the cache for weights and bias with zeros, matching the shape of the weights and biases respectively\n",
        "            layer.weight_cache =np.zeros_like(layer.weights)\n",
        "            layer.bias_cache =np.zeros_like(layer.biases)\n",
        "\n",
        "        # Update the cache by accumulating the squared gradients, This cache is used to adapt the learning rate for each parameter individually\n",
        "        layer.weight_cache+=layer.dweights**2\n",
        "        layer.bias_cache+=layer.dbiases**2\n",
        "\n",
        "        # Update the weights using New weight = old weight - (current learning rate * gradient) / (sqrt(weight_cache) + epsilon)\n",
        "        layer.weights+= -self.current_learning_rate * layer.dweights/(np.sqrt(layer.weight_cache)+self.epsilon)\n",
        "        layer.biases+= -self.current_learning_rate * layer.dbiases/(np.sqrt(layer.bias_cache)+self.epsilon)\n",
        "\n",
        "    def post_update_param(self):\n",
        "        self.iterations += 1  # Increment the iteration count after each parameter update"
      ],
      "metadata": {
        "id": "jmObraXOr8-H"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RMSprop:\n",
        "    def __init__(self,learning_rate=0.001,decay=0.,epsilon=1e-7,rho=0.9):\n",
        "        self.learning_rate=learning_rate\n",
        "        self.current_learning_rate = learning_rate  # Initialize the current learning rate (this may change over iterations due to decay)\n",
        "        self.decay = decay\n",
        "        self.iteration = 0 # Initialize the iteration counter\n",
        "        self.epsilon=epsilon\n",
        "        self.rho=rho\n",
        "\n",
        "    def pre_update_param(self):\n",
        "        # If decay is set, adjust the current learning rate based on the iteration number\n",
        "        if self.decay:\n",
        "            self.current_learning_rate=self.learning_rate*(1./(1.+self.decay*self.iteration)) # This applies a simple decay formula: lr = initial_lr / (1 + decay * iterations)\n",
        "\n",
        "    def update_param(self,layer):\n",
        "        # Check if the layer already has a cache for weights and biases; if not, initialize them\n",
        "        if not hasattr(layer,'weight_cache'):\n",
        "            # Initialize the cache for weights and bias with zeros, matching the shape of the weights and biases respectively\n",
        "            layer.weight_cache=np.zeros_like(layer.weights)\n",
        "            layer.bias_cache=np.zeros_like(layer.biases)\n",
        "\n",
        "        # Update the cache for weights using the RMSprop formula: cache = rho * cache + (1 - rho) * (dweights)^2\n",
        "        layer.weight_cache=self.rho*layer.weight_cache +(1-self.rho)*layer.dweights**2\n",
        "        layer.bias_cache=self.rho*layer.bias_cache +(1-self.rho)*layer.dbiases**2\n",
        "\n",
        "        # Update weights using the RMSprop update rule: weights = weights - lr * dweights / (sqrt(weight_cache) + epsilon)\n",
        "        layer.weights+= -self.current_learning_rate* layer.dweights/(np.sqrt(layer.weight_cache)+self.epsilon)\n",
        "        layer.biases+= -self.current_learning_rate* layer.dbiases/(np.sqrt(layer.bias_cache)+self.epsilon)\n",
        "\n",
        "    def post_update_param(self):\n",
        "        self.iteration+=1 # Increment the iteration count after each parameter update"
      ],
      "metadata": {
        "id": "6mlvIK9btbyx"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Adam:\n",
        "    def __init__(self,learning_rate=0.001,decay=0.,epsilon=1e-7,beta_1=0.9,beta_2=0.999):\n",
        "        self.learning_rate =learning_rate\n",
        "        self.current_learning_rate =learning_rate # Initialize the current learning rate (this may change over iterations due to decay)\n",
        "        self.decay =decay\n",
        "        self.iteration =0 # Initialize the iteration counter\n",
        "        self.epsilon =epsilon\n",
        "        self.beta_1 =beta_1 # Exponential decay rate for the first moment(momentum) estimates\n",
        "        self.beta_2 =beta_2 # Exponential decay rate for the second moment(cache) estimates\n",
        "\n",
        "    def pre_update_param(self):\n",
        "        # If decay is set, adjust the current learning rate based on the iteration number\n",
        "        if self.decay:\n",
        "            self.current_learning_rate = self.learning_rate*(1./(1. + self.decay*self.iteration)) # This applies a simple decay formula: lr = initial_lr / (1 + decay * iterations)\n",
        "\n",
        "    def update_param(self,layer):\n",
        "        # If the layer does not have momentum and cache variables, initialize them\n",
        "        if not hasattr(layer,'weight_cache'):\n",
        "            layer.weight_momentum =np.zeros_like(layer.weights)\n",
        "            layer.weight_cache =np.zeros_like(layer.weights)\n",
        "            layer.bias_momentum =np.zeros_like(layer.biases)\n",
        "            layer.bias_cache =np.zeros_like(layer.biases)\n",
        "\n",
        "        # momentum\n",
        "        # Update the first moment estimate (momentum): m_t = beta_1 * m_(t-1) + (1 - beta_1) * current gradient\n",
        "        layer.weight_momentum = self.beta_1*layer.weight_momentum + (1-self.beta_1)*layer.dweights\n",
        "        layer.bias_momentum = self.beta_1*layer.bias_momentum + (1-self.beta_1)*layer.dbiases\n",
        "\n",
        "        # Correct bias in the first moment: momentum_term = m_t / (1 - beta_1^(t)) , where t is the iteration\n",
        "        weight_momentum_corrected = layer.weight_momentum / (1-self.beta_1**(self.iteration+1))\n",
        "        bias_momentum_corrected = layer.bias_momentum / (1-self.beta_1**(self.iteration+1))\n",
        "\n",
        "        # cache\n",
        "        # Update the second moment estimate (cache): v_t = beta_2 * v_(t-1) + (1 - beta_2) * (current gradient)^2\n",
        "        layer.weight_cache=self.beta_2 * layer.weight_cache + (1-self.beta_2)*layer.dweights**2\n",
        "        layer.bias_cache=self.beta_2 * layer.bias_cache + (1-self.beta_2)*layer.dbiases**2\n",
        "\n",
        "        # Correct bias in the second moment: cache_term = v_t / (1 - beta_2^(t))\n",
        "        weight_cache_corrected= layer.weight_cache / (1-self.beta_2 **(self.iteration +1))\n",
        "        bias_cache_corrected= layer.bias_cache/(1-self.beta_2**(self.iteration +1))\n",
        "\n",
        "        ## SGD\n",
        "        # Update parameters: parameter = parameter - lr * (momentum_term / (sqrt(cache_term) + epsilon))\n",
        "        layer.weights+= -self.current_learning_rate * weight_momentum_corrected/ (np.sqrt(weight_cache_corrected)+self.epsilon)\n",
        "        layer.biases+= -self.current_learning_rate *bias_momentum_corrected/ (np.sqrt(bias_cache_corrected)+self.epsilon)\n",
        "\n",
        "\n",
        "\n",
        "    def post_update_param(self):\n",
        "        self.iteration+=1  # Increment the iteration count after each parameter update"
      ],
      "metadata": {
        "id": "HqPSq64QutYC"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('/content/diabetes.csv.xls')\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "PjtOfkU_xgJZ",
        "outputId": "450c316a-9f2c-4658-e2c1-d366652ebec7"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
              "0            6      148             72             35        0  33.6   \n",
              "1            1       85             66             29        0  26.6   \n",
              "2            8      183             64              0        0  23.3   \n",
              "3            1       89             66             23       94  28.1   \n",
              "4            0      137             40             35      168  43.1   \n",
              "\n",
              "   DiabetesPedigreeFunction  Age  Outcome  \n",
              "0                     0.627   50        1  \n",
              "1                     0.351   31        0  \n",
              "2                     0.672   32        1  \n",
              "3                     0.167   21        0  \n",
              "4                     2.288   33        1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f88ae0ac-0bda-41b1-9bf6-0923c692bed1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>BloodPressure</th>\n",
              "      <th>SkinThickness</th>\n",
              "      <th>Insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <th>Age</th>\n",
              "      <th>Outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>148</td>\n",
              "      <td>72</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>85</td>\n",
              "      <td>66</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>183</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>89</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>94</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>137</td>\n",
              "      <td>40</td>\n",
              "      <td>35</td>\n",
              "      <td>168</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f88ae0ac-0bda-41b1-9bf6-0923c692bed1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f88ae0ac-0bda-41b1-9bf6-0923c692bed1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f88ae0ac-0bda-41b1-9bf6-0923c692bed1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-c78e79e2-82a7-4ccc-aef6-28f9855ef15a\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c78e79e2-82a7-4ccc-aef6-28f9855ef15a')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-c78e79e2-82a7-4ccc-aef6-28f9855ef15a button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 768,\n  \"fields\": [\n    {\n      \"column\": \"Pregnancies\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 0,\n        \"max\": 17,\n        \"num_unique_values\": 17,\n        \"samples\": [\n          6,\n          1,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Glucose\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 31,\n        \"min\": 0,\n        \"max\": 199,\n        \"num_unique_values\": 136,\n        \"samples\": [\n          151,\n          101,\n          112\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"BloodPressure\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 19,\n        \"min\": 0,\n        \"max\": 122,\n        \"num_unique_values\": 47,\n        \"samples\": [\n          86,\n          46,\n          85\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SkinThickness\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 15,\n        \"min\": 0,\n        \"max\": 99,\n        \"num_unique_values\": 51,\n        \"samples\": [\n          7,\n          12,\n          48\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Insulin\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 115,\n        \"min\": 0,\n        \"max\": 846,\n        \"num_unique_values\": 186,\n        \"samples\": [\n          52,\n          41,\n          183\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"BMI\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7.8841603203754405,\n        \"min\": 0.0,\n        \"max\": 67.1,\n        \"num_unique_values\": 248,\n        \"samples\": [\n          19.9,\n          31.0,\n          38.1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"DiabetesPedigreeFunction\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.33132859501277484,\n        \"min\": 0.078,\n        \"max\": 2.42,\n        \"num_unique_values\": 517,\n        \"samples\": [\n          1.731,\n          0.426,\n          0.138\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 11,\n        \"min\": 21,\n        \"max\": 81,\n        \"num_unique_values\": 52,\n        \"samples\": [\n          60,\n          47,\n          72\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Outcome\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Statistical summary\n",
        "df.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "P0vTt4_6xxwi",
        "outputId": "10650d13-528b-4bcb-d28c-61aaa54189ce"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
              "count   768.000000  768.000000     768.000000     768.000000  768.000000   \n",
              "mean      3.845052  120.894531      69.105469      20.536458   79.799479   \n",
              "std       3.369578   31.972618      19.355807      15.952218  115.244002   \n",
              "min       0.000000    0.000000       0.000000       0.000000    0.000000   \n",
              "25%       1.000000   99.000000      62.000000       0.000000    0.000000   \n",
              "50%       3.000000  117.000000      72.000000      23.000000   30.500000   \n",
              "75%       6.000000  140.250000      80.000000      32.000000  127.250000   \n",
              "max      17.000000  199.000000     122.000000      99.000000  846.000000   \n",
              "\n",
              "              BMI  DiabetesPedigreeFunction         Age     Outcome  \n",
              "count  768.000000                768.000000  768.000000  768.000000  \n",
              "mean    31.992578                  0.471876   33.240885    0.348958  \n",
              "std      7.884160                  0.331329   11.760232    0.476951  \n",
              "min      0.000000                  0.078000   21.000000    0.000000  \n",
              "25%     27.300000                  0.243750   24.000000    0.000000  \n",
              "50%     32.000000                  0.372500   29.000000    0.000000  \n",
              "75%     36.600000                  0.626250   41.000000    1.000000  \n",
              "max     67.100000                  2.420000   81.000000    1.000000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-557760c5-6c2f-49e7-8b7e-2593920435a8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>BloodPressure</th>\n",
              "      <th>SkinThickness</th>\n",
              "      <th>Insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <th>Age</th>\n",
              "      <th>Outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>768.000000</td>\n",
              "      <td>768.000000</td>\n",
              "      <td>768.000000</td>\n",
              "      <td>768.000000</td>\n",
              "      <td>768.000000</td>\n",
              "      <td>768.000000</td>\n",
              "      <td>768.000000</td>\n",
              "      <td>768.000000</td>\n",
              "      <td>768.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>3.845052</td>\n",
              "      <td>120.894531</td>\n",
              "      <td>69.105469</td>\n",
              "      <td>20.536458</td>\n",
              "      <td>79.799479</td>\n",
              "      <td>31.992578</td>\n",
              "      <td>0.471876</td>\n",
              "      <td>33.240885</td>\n",
              "      <td>0.348958</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>3.369578</td>\n",
              "      <td>31.972618</td>\n",
              "      <td>19.355807</td>\n",
              "      <td>15.952218</td>\n",
              "      <td>115.244002</td>\n",
              "      <td>7.884160</td>\n",
              "      <td>0.331329</td>\n",
              "      <td>11.760232</td>\n",
              "      <td>0.476951</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.078000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>99.000000</td>\n",
              "      <td>62.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>27.300000</td>\n",
              "      <td>0.243750</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>3.000000</td>\n",
              "      <td>117.000000</td>\n",
              "      <td>72.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>30.500000</td>\n",
              "      <td>32.000000</td>\n",
              "      <td>0.372500</td>\n",
              "      <td>29.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>6.000000</td>\n",
              "      <td>140.250000</td>\n",
              "      <td>80.000000</td>\n",
              "      <td>32.000000</td>\n",
              "      <td>127.250000</td>\n",
              "      <td>36.600000</td>\n",
              "      <td>0.626250</td>\n",
              "      <td>41.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>17.000000</td>\n",
              "      <td>199.000000</td>\n",
              "      <td>122.000000</td>\n",
              "      <td>99.000000</td>\n",
              "      <td>846.000000</td>\n",
              "      <td>67.100000</td>\n",
              "      <td>2.420000</td>\n",
              "      <td>81.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-557760c5-6c2f-49e7-8b7e-2593920435a8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-557760c5-6c2f-49e7-8b7e-2593920435a8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-557760c5-6c2f-49e7-8b7e-2593920435a8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1d778860-0ea0-471d-ad0b-8fb3002139b4\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1d778860-0ea0-471d-ad0b-8fb3002139b4')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1d778860-0ea0-471d-ad0b-8fb3002139b4 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"Pregnancies\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 269.85223453356366,\n        \"min\": 0.0,\n        \"max\": 768.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          3.8450520833333335,\n          3.0,\n          768.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Glucose\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 243.73802348295857,\n        \"min\": 0.0,\n        \"max\": 768.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          120.89453125,\n          117.0,\n          768.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"BloodPressure\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 252.85250535810619,\n        \"min\": 0.0,\n        \"max\": 768.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          69.10546875,\n          72.0,\n          768.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SkinThickness\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 263.7684730531098,\n        \"min\": 0.0,\n        \"max\": 768.0,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          768.0,\n          20.536458333333332,\n          32.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Insulin\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 350.26059167945886,\n        \"min\": 0.0,\n        \"max\": 846.0,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          768.0,\n          79.79947916666667,\n          127.25\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"BMI\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 262.05117817552093,\n        \"min\": 0.0,\n        \"max\": 768.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          31.992578124999998,\n          32.0,\n          768.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"DiabetesPedigreeFunction\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 271.3005221658502,\n        \"min\": 0.078,\n        \"max\": 768.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.47187630208333325,\n          0.3725,\n          768.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 260.1941178528413,\n        \"min\": 11.76023154067868,\n        \"max\": 768.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          33.240885416666664,\n          29.0,\n          768.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Outcome\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 271.3865920388932,\n        \"min\": 0.0,\n        \"max\": 768.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.3489583333333333,\n          1.0,\n          0.4769513772427971\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing values in each column\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "HfaX4fjOyK5P",
        "outputId": "3ebcb76f-0326-4eb6-8ee3-9efcb35d4477"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pregnancies                 0\n",
              "Glucose                     0\n",
              "BloodPressure               0\n",
              "SkinThickness               0\n",
              "Insulin                     0\n",
              "BMI                         0\n",
              "DiabetesPedigreeFunction    0\n",
              "Age                         0\n",
              "Outcome                     0\n",
              "dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Pregnancies</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Glucose</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>BloodPressure</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SkinThickness</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Insulin</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>BMI</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Age</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Outcome</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# contains duplicate\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hL-YWjSDyTdY",
        "outputId": "872ac681-5b9f-4197-e5c6-4519e9034c48"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x=df.drop(columns=['Outcome'],axis=1)\n",
        "y=df['Outcome']"
      ],
      "metadata": {
        "id": "LQIJOdh-yY-Y"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=49)\n",
        "\n",
        "x_train = x_train.values\n",
        "x_test = x_test.values\n",
        "y_train = y_train.values\n",
        "y_test = y_test.values"
      ],
      "metadata": {
        "id": "k6YZNUqQymZ6"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set up the network layers:"
      ],
      "metadata": {
        "id": "VogGO87RzgDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First Dense layer with 8 input features to 64 neurons, with L2 regularization on both weights and biases\n",
        "dense1=Dense(8,64,weight_regularizer_l2=5e-4,bias_regularizer_l2=5e-4)\n",
        "# ReLU activation layer applied after dense1\n",
        "act1=ReLU()\n",
        "# Dropout layer with a dropout rate of 10% to reduce overfitting\n",
        "drop1=Dropout(0.1)\n",
        "# Second Dense layer with 64 inputs (from the previous layer) to 2 output neurons (for binary classification)\n",
        "dense2=Dense(64,2)\n",
        "# Combined Softmax activation and Categorical Cross-Entropy loss\n",
        "loss_act=Softmax_CrossEntropy()"
      ],
      "metadata": {
        "id": "HH4Njaf6zF01"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Adagrad Optimizer"
      ],
      "metadata": {
        "id": "XNxKx0Io02vB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "opt=Adagrad(decay=1e-4)\n",
        "\n",
        "#Training\n",
        "for i in range(10001):\n",
        "    # -------- Forward Pass --------\n",
        "    dense1.forward(x_train) # First dense layer processing\n",
        "    act1.forward(dense1.output) # ReLU activation on dense1's output\n",
        "    drop1.forward(act1.output) # Dropout applied to the activated output\n",
        "    dense2.forward(drop1.output) # Second dense layer processing\n",
        "\n",
        "    # Computing the data loss using softmax and cross-entropy\n",
        "    data_loss=loss_act.forward(dense2.output,y_train)\n",
        "\n",
        "    # Computing the regularization loss from both dense layers\n",
        "    regularization_loss=(loss_act.loss.regularization_loss(dense1) + loss_act.loss.regularization_loss(dense2))\n",
        "\n",
        "    # Total loss is the sum of the data loss and the regularization loss\n",
        "    loss=data_loss+regularization_loss\n",
        "\n",
        "    # -------- Accuracy Calculation --------\n",
        "    # Determine the predicted classes by taking the index of the maximum probability\n",
        "    pred=np.argmax(loss_act.output,axis=1)\n",
        "    # If y_train is one-hot encoded, convert it to class indices\n",
        "    if len(y_train.shape)==2:\n",
        "        y_train=np.argmax(y_train,axis=1)\n",
        "    # Calculating the accuracy as the proportion of correct predictions\n",
        "    acc=np.mean(pred==y_train)\n",
        "\n",
        "    if not i%100:\n",
        "        print(f'epoch: {i}, acc: {acc:.3f}, loss: {loss:.3f} (data_loss: {data_loss:.3f}, reg_loss: {regularization_loss:.3f}), lr: {opt.current_learning_rate} ')\n",
        "\n",
        "    # -------- Backward Pass --------\n",
        "    # Backpropagate through the loss layer to get the initial gradient\n",
        "    loss_act.backward(loss_act.output,y_train)\n",
        "    dense2.backward(loss_act.dinputs) # Backprop through second dense layer\n",
        "    drop1.backward(dense2.dinputs) # Backprop through dropout layer\n",
        "    act1.backward(drop1.dinputs) # Backprop through ReLU activation\n",
        "    dense1.backward(act1.dinputs) # Backprop through first dense layer\n",
        "\n",
        "    # -------- Parameter Update --------\n",
        "    opt.pre_update_param() # Adjust the learning rate if decay is used\n",
        "    opt.update_param(dense1) # Update parameters for the first dense layer\n",
        "    opt.update_param(dense2) # Update parameters for the second dense layer\n",
        "    opt.post_update_param() # Increment the iteration counter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p94s7M0K1Bfe",
        "outputId": "7a6b412a-6ccf-460d-d76c-2ede05abf8a4"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 1.0 \n",
            "epoch: 100, acc: 0.378, loss: 9.826 (data_loss: 9.712, reg_loss: 0.114), lr: 0.9901970492127933 \n",
            "epoch: 200, acc: 0.689, loss: 0.708 (data_loss: 0.599, reg_loss: 0.109), lr: 0.9804882831650161 \n",
            "epoch: 300, acc: 0.687, loss: 0.690 (data_loss: 0.587, reg_loss: 0.102), lr: 0.9709680551509855 \n",
            "epoch: 400, acc: 0.694, loss: 0.671 (data_loss: 0.572, reg_loss: 0.098), lr: 0.9616309260505818 \n",
            "epoch: 500, acc: 0.699, loss: 0.677 (data_loss: 0.582, reg_loss: 0.095), lr: 0.9524716639679969 \n",
            "epoch: 600, acc: 0.712, loss: 0.659 (data_loss: 0.566, reg_loss: 0.092), lr: 0.9434852344560807 \n",
            "epoch: 700, acc: 0.700, loss: 0.660 (data_loss: 0.570, reg_loss: 0.090), lr: 0.9346667912889054 \n",
            "epoch: 800, acc: 0.707, loss: 0.657 (data_loss: 0.569, reg_loss: 0.088), lr: 0.9260116677470135 \n",
            "epoch: 900, acc: 0.702, loss: 0.650 (data_loss: 0.563, reg_loss: 0.087), lr: 0.9175153683824203 \n",
            "epoch: 1000, acc: 0.713, loss: 0.632 (data_loss: 0.547, reg_loss: 0.085), lr: 0.9091735612328392 \n",
            "epoch: 1100, acc: 0.699, loss: 0.640 (data_loss: 0.556, reg_loss: 0.084), lr: 0.9009820704567978 \n",
            "epoch: 1200, acc: 0.704, loss: 0.631 (data_loss: 0.548, reg_loss: 0.083), lr: 0.892936869363336 \n",
            "epoch: 1300, acc: 0.712, loss: 0.635 (data_loss: 0.554, reg_loss: 0.081), lr: 0.8850340738118416 \n",
            "epoch: 1400, acc: 0.710, loss: 0.640 (data_loss: 0.560, reg_loss: 0.080), lr: 0.8772699359592947 \n",
            "epoch: 1500, acc: 0.710, loss: 0.636 (data_loss: 0.556, reg_loss: 0.080), lr: 0.8696408383337683 \n",
            "epoch: 1600, acc: 0.700, loss: 0.635 (data_loss: 0.556, reg_loss: 0.079), lr: 0.8621432882145013 \n",
            "epoch: 1700, acc: 0.697, loss: 0.633 (data_loss: 0.555, reg_loss: 0.078), lr: 0.8547739123001966 \n",
            "epoch: 1800, acc: 0.708, loss: 0.619 (data_loss: 0.542, reg_loss: 0.077), lr: 0.8475294516484448 \n",
            "epoch: 1900, acc: 0.707, loss: 0.633 (data_loss: 0.556, reg_loss: 0.076), lr: 0.8404067568703253 \n",
            "epoch: 2000, acc: 0.699, loss: 0.627 (data_loss: 0.551, reg_loss: 0.076), lr: 0.8334027835652972 \n",
            "epoch: 2100, acc: 0.705, loss: 0.624 (data_loss: 0.549, reg_loss: 0.075), lr: 0.8265145879824779 \n",
            "epoch: 2200, acc: 0.717, loss: 0.620 (data_loss: 0.545, reg_loss: 0.074), lr: 0.8197393228953193 \n",
            "epoch: 2300, acc: 0.700, loss: 0.622 (data_loss: 0.548, reg_loss: 0.074), lr: 0.8130742336775347 \n",
            "epoch: 2400, acc: 0.707, loss: 0.617 (data_loss: 0.544, reg_loss: 0.073), lr: 0.8065166545689169 \n",
            "epoch: 2500, acc: 0.704, loss: 0.620 (data_loss: 0.547, reg_loss: 0.073), lr: 0.8000640051204096 \n",
            "epoch: 2600, acc: 0.713, loss: 0.607 (data_loss: 0.535, reg_loss: 0.072), lr: 0.7937137868084768 \n",
            "epoch: 2700, acc: 0.723, loss: 0.617 (data_loss: 0.545, reg_loss: 0.072), lr: 0.7874635798094338 \n",
            "epoch: 2800, acc: 0.718, loss: 0.616 (data_loss: 0.544, reg_loss: 0.071), lr: 0.7813110399249941 \n",
            "epoch: 2900, acc: 0.707, loss: 0.606 (data_loss: 0.536, reg_loss: 0.071), lr: 0.7752538956508256 \n",
            "epoch: 3000, acc: 0.710, loss: 0.615 (data_loss: 0.544, reg_loss: 0.071), lr: 0.7692899453804138 \n",
            "epoch: 3100, acc: 0.715, loss: 0.609 (data_loss: 0.539, reg_loss: 0.070), lr: 0.7634170547370028 \n",
            "epoch: 3200, acc: 0.718, loss: 0.607 (data_loss: 0.537, reg_loss: 0.070), lr: 0.7576331540268202 \n",
            "epoch: 3300, acc: 0.720, loss: 0.594 (data_loss: 0.525, reg_loss: 0.069), lr: 0.7519362358072035 \n",
            "epoch: 3400, acc: 0.718, loss: 0.607 (data_loss: 0.538, reg_loss: 0.069), lr: 0.7463243525636241 \n",
            "epoch: 3500, acc: 0.712, loss: 0.612 (data_loss: 0.543, reg_loss: 0.069), lr: 0.7407956144899621 \n",
            "epoch: 3600, acc: 0.715, loss: 0.602 (data_loss: 0.533, reg_loss: 0.068), lr: 0.735348187366718 \n",
            "epoch: 3700, acc: 0.718, loss: 0.606 (data_loss: 0.538, reg_loss: 0.068), lr: 0.7299802905321557 \n",
            "epoch: 3800, acc: 0.721, loss: 0.610 (data_loss: 0.542, reg_loss: 0.068), lr: 0.7246901949416624 \n",
            "epoch: 3900, acc: 0.715, loss: 0.610 (data_loss: 0.542, reg_loss: 0.067), lr: 0.7194762213108857 \n",
            "epoch: 4000, acc: 0.710, loss: 0.609 (data_loss: 0.542, reg_loss: 0.067), lr: 0.7143367383384527 \n",
            "epoch: 4100, acc: 0.715, loss: 0.604 (data_loss: 0.537, reg_loss: 0.067), lr: 0.7092701610043266 \n",
            "epoch: 4200, acc: 0.715, loss: 0.600 (data_loss: 0.533, reg_loss: 0.066), lr: 0.7042749489400663 \n",
            "epoch: 4300, acc: 0.692, loss: 0.603 (data_loss: 0.537, reg_loss: 0.066), lr: 0.6993496048674733 \n",
            "epoch: 4400, acc: 0.718, loss: 0.587 (data_loss: 0.521, reg_loss: 0.066), lr: 0.6944926731022988 \n",
            "epoch: 4500, acc: 0.717, loss: 0.595 (data_loss: 0.529, reg_loss: 0.066), lr: 0.6897027381198704 \n",
            "epoch: 4600, acc: 0.708, loss: 0.601 (data_loss: 0.536, reg_loss: 0.065), lr: 0.6849784231796698 \n",
            "epoch: 4700, acc: 0.723, loss: 0.584 (data_loss: 0.519, reg_loss: 0.065), lr: 0.6803183890060548 \n",
            "epoch: 4800, acc: 0.723, loss: 0.586 (data_loss: 0.521, reg_loss: 0.065), lr: 0.6757213325224677 \n",
            "epoch: 4900, acc: 0.712, loss: 0.596 (data_loss: 0.532, reg_loss: 0.065), lr: 0.6711859856366199 \n",
            "epoch: 5000, acc: 0.712, loss: 0.590 (data_loss: 0.525, reg_loss: 0.065), lr: 0.6667111140742716 \n",
            "epoch: 5100, acc: 0.721, loss: 0.603 (data_loss: 0.539, reg_loss: 0.064), lr: 0.6622955162593549 \n",
            "epoch: 5200, acc: 0.715, loss: 0.606 (data_loss: 0.541, reg_loss: 0.064), lr: 0.6579380222383051 \n",
            "epoch: 5300, acc: 0.718, loss: 0.590 (data_loss: 0.526, reg_loss: 0.064), lr: 0.6536374926465782 \n",
            "epoch: 5400, acc: 0.704, loss: 0.600 (data_loss: 0.536, reg_loss: 0.064), lr: 0.649392817715436 \n",
            "epoch: 5500, acc: 0.715, loss: 0.581 (data_loss: 0.517, reg_loss: 0.064), lr: 0.6452029163171817 \n",
            "epoch: 5600, acc: 0.712, loss: 0.590 (data_loss: 0.526, reg_loss: 0.064), lr: 0.6410667350471184 \n",
            "epoch: 5700, acc: 0.705, loss: 0.588 (data_loss: 0.525, reg_loss: 0.063), lr: 0.6369832473405949 \n",
            "epoch: 5800, acc: 0.700, loss: 0.616 (data_loss: 0.553, reg_loss: 0.063), lr: 0.6329514526235838 \n",
            "epoch: 5900, acc: 0.713, loss: 0.588 (data_loss: 0.525, reg_loss: 0.063), lr: 0.6289703754953141 \n",
            "epoch: 6000, acc: 0.708, loss: 0.594 (data_loss: 0.531, reg_loss: 0.063), lr: 0.6250390649415589 \n",
            "epoch: 6100, acc: 0.708, loss: 0.589 (data_loss: 0.526, reg_loss: 0.063), lr: 0.6211565935772407 \n",
            "epoch: 6200, acc: 0.718, loss: 0.575 (data_loss: 0.513, reg_loss: 0.063), lr: 0.6173220569170937 \n",
            "epoch: 6300, acc: 0.710, loss: 0.593 (data_loss: 0.530, reg_loss: 0.063), lr: 0.6135345726731701 \n",
            "epoch: 6400, acc: 0.708, loss: 0.581 (data_loss: 0.519, reg_loss: 0.063), lr: 0.6097932800780536 \n",
            "epoch: 6500, acc: 0.705, loss: 0.586 (data_loss: 0.524, reg_loss: 0.062), lr: 0.6060973392326807 \n",
            "epoch: 6600, acc: 0.712, loss: 0.582 (data_loss: 0.520, reg_loss: 0.062), lr: 0.6024459304777396 \n",
            "epoch: 6700, acc: 0.704, loss: 0.601 (data_loss: 0.539, reg_loss: 0.062), lr: 0.5988382537876519 \n",
            "epoch: 6800, acc: 0.708, loss: 0.576 (data_loss: 0.514, reg_loss: 0.062), lr: 0.5952735281862016 \n",
            "epoch: 6900, acc: 0.725, loss: 0.587 (data_loss: 0.524, reg_loss: 0.062), lr: 0.5917509911829102 \n",
            "epoch: 7000, acc: 0.717, loss: 0.580 (data_loss: 0.518, reg_loss: 0.062), lr: 0.5882698982293076 \n",
            "epoch: 7100, acc: 0.695, loss: 0.616 (data_loss: 0.554, reg_loss: 0.062), lr: 0.5848295221942803 \n",
            "epoch: 7200, acc: 0.735, loss: 0.577 (data_loss: 0.515, reg_loss: 0.062), lr: 0.5814291528577243 \n",
            "epoch: 7300, acc: 0.728, loss: 0.571 (data_loss: 0.510, reg_loss: 0.062), lr: 0.5780680964217585 \n",
            "epoch: 7400, acc: 0.728, loss: 0.578 (data_loss: 0.516, reg_loss: 0.062), lr: 0.5747456750387954 \n",
            "epoch: 7500, acc: 0.730, loss: 0.578 (data_loss: 0.516, reg_loss: 0.062), lr: 0.5714612263557918 \n",
            "epoch: 7600, acc: 0.715, loss: 0.580 (data_loss: 0.519, reg_loss: 0.061), lr: 0.5682141030740383 \n",
            "epoch: 7700, acc: 0.735, loss: 0.568 (data_loss: 0.507, reg_loss: 0.061), lr: 0.5650036725238714 \n",
            "epoch: 7800, acc: 0.730, loss: 0.574 (data_loss: 0.513, reg_loss: 0.061), lr: 0.5618293162537221 \n",
            "epoch: 7900, acc: 0.717, loss: 0.597 (data_loss: 0.536, reg_loss: 0.061), lr: 0.5586904296329404 \n",
            "epoch: 8000, acc: 0.725, loss: 0.586 (data_loss: 0.525, reg_loss: 0.061), lr: 0.5555864214678593 \n",
            "epoch: 8100, acc: 0.726, loss: 0.573 (data_loss: 0.512, reg_loss: 0.061), lr: 0.5525167136305873 \n",
            "epoch: 8200, acc: 0.721, loss: 0.573 (data_loss: 0.512, reg_loss: 0.061), lr: 0.5494807407000385 \n",
            "epoch: 8300, acc: 0.728, loss: 0.571 (data_loss: 0.510, reg_loss: 0.061), lr: 0.5464779496147331 \n",
            "epoch: 8400, acc: 0.720, loss: 0.585 (data_loss: 0.524, reg_loss: 0.061), lr: 0.5435077993369205 \n",
            "epoch: 8500, acc: 0.735, loss: 0.571 (data_loss: 0.510, reg_loss: 0.061), lr: 0.5405697605275961 \n",
            "epoch: 8600, acc: 0.712, loss: 0.589 (data_loss: 0.528, reg_loss: 0.061), lr: 0.5376633152320017 \n",
            "epoch: 8700, acc: 0.710, loss: 0.585 (data_loss: 0.524, reg_loss: 0.061), lr: 0.5347879565752179 \n",
            "epoch: 8800, acc: 0.726, loss: 0.563 (data_loss: 0.502, reg_loss: 0.061), lr: 0.5319431884674717 \n",
            "epoch: 8900, acc: 0.726, loss: 0.572 (data_loss: 0.512, reg_loss: 0.061), lr: 0.5291285253188 \n",
            "epoch: 9000, acc: 0.725, loss: 0.578 (data_loss: 0.517, reg_loss: 0.061), lr: 0.5263434917627243 \n",
            "epoch: 9100, acc: 0.723, loss: 0.572 (data_loss: 0.511, reg_loss: 0.061), lr: 0.5235876223886068 \n",
            "epoch: 9200, acc: 0.721, loss: 0.573 (data_loss: 0.513, reg_loss: 0.060), lr: 0.5208604614823689 \n",
            "epoch: 9300, acc: 0.728, loss: 0.569 (data_loss: 0.509, reg_loss: 0.060), lr: 0.5181615627752734 \n",
            "epoch: 9400, acc: 0.718, loss: 0.574 (data_loss: 0.513, reg_loss: 0.060), lr: 0.5154904892004742 \n",
            "epoch: 9500, acc: 0.717, loss: 0.579 (data_loss: 0.518, reg_loss: 0.060), lr: 0.5128468126570593 \n",
            "epoch: 9600, acc: 0.725, loss: 0.571 (data_loss: 0.511, reg_loss: 0.060), lr: 0.5102301137813153 \n",
            "epoch: 9700, acc: 0.725, loss: 0.577 (data_loss: 0.516, reg_loss: 0.060), lr: 0.5076399817249606 \n",
            "epoch: 9800, acc: 0.726, loss: 0.572 (data_loss: 0.512, reg_loss: 0.060), lr: 0.5050760139400979 \n",
            "epoch: 9900, acc: 0.726, loss: 0.572 (data_loss: 0.512, reg_loss: 0.060), lr: 0.5025378159706518 \n",
            "epoch: 10000, acc: 0.721, loss: 0.566 (data_loss: 0.506, reg_loss: 0.060), lr: 0.5000250012500626 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing\n",
        "# Forward pass on the test set\n",
        "dense1.forward(x_test)\n",
        "act1.forward(dense1.output)\n",
        "dense2.forward(act1.output)\n",
        "\n",
        "# Computing the loss on the test set\n",
        "loss=loss_act.forward(dense2.output,y_test)\n",
        "\n",
        "pred=np.argmax(loss_act.output,axis=1)\n",
        "if len(y_test.shape)==2:\n",
        "    y_test=np.argmax(y,axis=1)\n",
        "acc=np.mean(pred==y_test)\n",
        "\n",
        "print(f'Validation accuracy of Adagrad optimizer\\nacc: {acc:.3f}, loss: {loss:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjonwX3W2a5Z",
        "outputId": "6e65889f-85ae-42e5-c2ca-9f0414c2346b"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy of Adagrad optimizer\n",
            "acc: 0.623, loss: 0.786\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### SGD Optimizer"
      ],
      "metadata": {
        "id": "3wxTNNQs3OKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "opt=SGD(decay=1e-3,momentum=0.9)\n",
        "\n",
        "#Training\n",
        "for i in range(10001):\n",
        "    # -------- Forward Pass --------\n",
        "    dense1.forward(x_train) # First dense layer processing\n",
        "    act1.forward(dense1.output) # ReLU activation on dense1's output\n",
        "    drop1.forward(act1.output) # Dropout applied to the activated output\n",
        "    dense2.forward(drop1.output) # Second dense layer processing\n",
        "\n",
        "    # Computing the data loss using softmax and cross-entropy\n",
        "    data_loss=loss_act.forward(dense2.output,y_train)\n",
        "\n",
        "    # Computing the regularization loss from both dense layers\n",
        "    regularization_loss=(loss_act.loss.regularization_loss(dense1) + loss_act.loss.regularization_loss(dense2))\n",
        "\n",
        "    # Total loss is the sum of the data loss and the regularization loss\n",
        "    loss=data_loss+regularization_loss\n",
        "\n",
        "    # -------- Accuracy Calculation --------\n",
        "    # Determine the predicted classes by taking the index of the maximum probability\n",
        "    pred=np.argmax(loss_act.output,axis=1)\n",
        "    # If y_train is one-hot encoded, convert it to class indices\n",
        "    if len(y_train.shape)==2:\n",
        "        y_train=np.argmax(y_train,axis=1)\n",
        "    # Calculating the accuracy as the proportion of correct predictions\n",
        "    acc=np.mean(pred==y_train)\n",
        "\n",
        "    if not i%100:\n",
        "        print(f'epoch: {i}, acc: {acc:.3f}, loss: {loss:.3f} (data_loss: {data_loss:.3f}, reg_loss: {regularization_loss:.3f}), lr: {opt.current_learning_rate} ')\n",
        "\n",
        "    # -------- Backward Pass --------\n",
        "    # Backpropagate through the loss layer to get the initial gradient\n",
        "    loss_act.backward(loss_act.output,y_train)\n",
        "    dense2.backward(loss_act.dinputs) # Backprop through second dense layer\n",
        "    drop1.backward(dense2.dinputs) # Backprop through dropout layer\n",
        "    act1.backward(drop1.dinputs) # Backprop through ReLU activation\n",
        "    dense1.backward(act1.dinputs) # Backprop through first dense layer\n",
        "\n",
        "    # -------- Parameter Update --------\n",
        "    opt.pre_update_param() # Adjust the learning rate if decay is used\n",
        "    opt.update_param(dense1) # Update parameters for the first dense layer\n",
        "    opt.update_param(dense2) # Update parameters for the second dense layer\n",
        "    opt.post_update_param() # Increment the iteration counter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBgaRdNs3FPQ",
        "outputId": "f589bccf-9863-4ce2-d28b-bb95d93207b9"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 1.0 \n",
            "epoch: 100, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.9099181073703367 \n",
            "epoch: 200, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.8340283569641367 \n",
            "epoch: 300, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.7698229407236336 \n",
            "epoch: 400, acc: 0.334, loss: 481.369 (data_loss: 0.715, reg_loss: 480.654), lr: 0.7147962830593281 \n",
            "epoch: 500, acc: 0.666, loss: 295.821 (data_loss: 0.637, reg_loss: 295.184), lr: 0.66711140760507 \n",
            "epoch: 600, acc: 0.666, loss: 74.021 (data_loss: 0.637, reg_loss: 73.384), lr: 0.6253908692933083 \n",
            "epoch: 700, acc: 0.666, loss: 20.617 (data_loss: 0.637, reg_loss: 19.980), lr: 0.5885815185403178 \n",
            "epoch: 800, acc: 0.666, loss: 6.529 (data_loss: 0.637, reg_loss: 5.892), lr: 0.5558643690939411 \n",
            "epoch: 900, acc: 0.666, loss: 2.501 (data_loss: 0.637, reg_loss: 1.865), lr: 0.526592943654555 \n",
            "epoch: 1000, acc: 0.666, loss: 1.265 (data_loss: 0.637, reg_loss: 0.628), lr: 0.5002501250625312 \n",
            "epoch: 1100, acc: 0.666, loss: 0.861 (data_loss: 0.637, reg_loss: 0.224), lr: 0.4764173415912339 \n",
            "epoch: 1200, acc: 0.666, loss: 0.721 (data_loss: 0.637, reg_loss: 0.084), lr: 0.45475216007276037 \n",
            "epoch: 1300, acc: 0.666, loss: 0.670 (data_loss: 0.637, reg_loss: 0.033), lr: 0.43497172683775553 \n",
            "epoch: 1400, acc: 0.666, loss: 0.650 (data_loss: 0.637, reg_loss: 0.014), lr: 0.4168403501458941 \n",
            "epoch: 1500, acc: 0.666, loss: 0.643 (data_loss: 0.637, reg_loss: 0.006), lr: 0.4001600640256102 \n",
            "epoch: 1600, acc: 0.666, loss: 0.639 (data_loss: 0.637, reg_loss: 0.003), lr: 0.3847633705271258 \n",
            "epoch: 1700, acc: 0.666, loss: 0.638 (data_loss: 0.637, reg_loss: 0.001), lr: 0.3705075954057058 \n",
            "epoch: 1800, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.001), lr: 0.35727045373347627 \n",
            "epoch: 1900, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.3449465332873405 \n",
            "epoch: 2000, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.33344448149383127 \n",
            "epoch: 2100, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.32268473701193934 \n",
            "epoch: 2200, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.31259768677711786 \n",
            "epoch: 2300, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.3031221582297666 \n",
            "epoch: 2400, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.29420417769932333 \n",
            "epoch: 2500, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.2857959416976279 \n",
            "epoch: 2600, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.2778549597110308 \n",
            "epoch: 2700, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.2703433360367667 \n",
            "epoch: 2800, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.26322716504343247 \n",
            "epoch: 2900, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.25647601949217746 \n",
            "epoch: 3000, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.25006251562890724 \n",
            "epoch: 3100, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.2439619419370578 \n",
            "epoch: 3200, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.23815194093831865 \n",
            "epoch: 3300, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.23261223540358225 \n",
            "epoch: 3400, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.22732439190725165 \n",
            "epoch: 3500, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.22227161591464767 \n",
            "epoch: 3600, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.21743857360295715 \n",
            "epoch: 3700, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.21281123643328367 \n",
            "epoch: 3800, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.20837674515524068 \n",
            "epoch: 3900, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.20412329046744235 \n",
            "epoch: 4000, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.2000400080016003 \n",
            "epoch: 4100, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.19611688566385566 \n",
            "epoch: 4200, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.19234468166955185 \n",
            "epoch: 4300, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.18871485185884126 \n",
            "epoch: 4400, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.18521948508983144 \n",
            "epoch: 4500, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.18185124568103292 \n",
            "epoch: 4600, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.1786033220217896 \n",
            "epoch: 4700, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.1754693805930865 \n",
            "epoch: 4800, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.17244352474564578 \n",
            "epoch: 4900, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.16952025767079165 \n",
            "epoch: 5000, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.16669444907484582 \n",
            "epoch: 5100, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.16396130513198884 \n",
            "epoch: 5200, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.16131634134537828 \n",
            "epoch: 5300, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.15875535799333226 \n",
            "epoch: 5400, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.1562744178777934 \n",
            "epoch: 5500, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.15386982612709646 \n",
            "epoch: 5600, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.15153811183512653 \n",
            "epoch: 5700, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.14927601134497687 \n",
            "epoch: 5800, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.14708045300779526 \n",
            "epoch: 5900, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.14494854326714016 \n",
            "epoch: 6000, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.1428775539362766 \n",
            "epoch: 6100, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.1408649105507818 \n",
            "epoch: 6200, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.13890818169190167 \n",
            "epoch: 6300, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.13700506918755992 \n",
            "epoch: 6400, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.13515339910798757 \n",
            "epoch: 6500, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.13335111348179757 \n",
            "epoch: 6600, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.13159626266614027 \n",
            "epoch: 6700, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.12988699831146902 \n",
            "epoch: 6800, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.12822156686754713 \n",
            "epoch: 6900, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.126598303582732 \n",
            "epoch: 7000, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.12501562695336915 \n",
            "epoch: 7100, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.12347203358439313 \n",
            "epoch: 7200, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.12196609342602757 \n",
            "epoch: 7300, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.12049644535486204 \n",
            "epoch: 7400, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.11906179307060363 \n",
            "epoch: 7500, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.11766090128250381 \n",
            "epoch: 7600, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.11629259216187929 \n",
            "epoch: 7700, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.11495574203931487 \n",
            "epoch: 7800, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.11364927832708263 \n",
            "epoch: 7900, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.11237217664906168 \n",
            "epoch: 8000, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.11112345816201799 \n",
            "epoch: 8100, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.10990218705352237 \n",
            "epoch: 8200, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.10870746820306555 \n",
            "epoch: 8300, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.1075384449940854 \n",
            "epoch: 8400, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.10639429726566654 \n",
            "epoch: 8500, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.10527423939362038 \n",
            "epoch: 8600, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.10417751849150952 \n",
            "epoch: 8700, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.10310341272296113 \n",
            "epoch: 8800, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.1020512297173181 \n",
            "epoch: 8900, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.10102030508132134 \n",
            "epoch: 9000, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.1000100010001 \n",
            "epoch: 9100, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.09901970492127933 \n",
            "epoch: 9200, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.09804882831650162 \n",
            "epoch: 9300, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.09709680551509856 \n",
            "epoch: 9400, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.09616309260505818 \n",
            "epoch: 9500, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.09524716639679968 \n",
            "epoch: 9600, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.09434852344560807 \n",
            "epoch: 9700, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.09346667912889055 \n",
            "epoch: 9800, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.09260116677470137 \n",
            "epoch: 9900, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.09175153683824203 \n",
            "epoch: 10000, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.09091735612328393 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing\n",
        "# Forward pass on the test set\n",
        "dense1.forward(x_test)\n",
        "act1.forward(dense1.output)\n",
        "dense2.forward(act1.output)\n",
        "\n",
        "# Computing the loss on the test set\n",
        "loss=loss_act.forward(dense2.output,y_test)\n",
        "\n",
        "pred=np.argmax(loss_act.output,axis=1)\n",
        "if len(y_test.shape)==2:\n",
        "    y_test=np.argmax(y,axis=1)\n",
        "acc=np.mean(pred==y_test)\n",
        "\n",
        "print(f'Validation accuracy of SGD optimizer\\n acc: {acc:.3f}, loss: {loss:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNNqM0cU3Y9-",
        "outputId": "63d52545-fe26-45e4-c1a9-f9ebfdb0ef3f"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy of SGD optimizer\n",
            " acc: 0.591, loss: 0.689\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### RMSprop Optimizer"
      ],
      "metadata": {
        "id": "Sbc83b9z3tVd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "opt=RMSprop(learning_rate=0.02,decay=1e-5,rho=0.999)\n",
        "\n",
        "#Training\n",
        "for i in range(10001):\n",
        "    # -------- Forward Pass --------\n",
        "    dense1.forward(x_train) # First dense layer processing\n",
        "    act1.forward(dense1.output) # ReLU activation on dense1's output\n",
        "    drop1.forward(act1.output) # Dropout applied to the activated output\n",
        "    dense2.forward(drop1.output) # Second dense layer processing\n",
        "\n",
        "    # Computing the data loss using softmax and cross-entropy\n",
        "    data_loss=loss_act.forward(dense2.output,y_train)\n",
        "\n",
        "    # Computing the regularization loss from both dense layers\n",
        "    regularization_loss=(loss_act.loss.regularization_loss(dense1) + loss_act.loss.regularization_loss(dense2))\n",
        "\n",
        "    # Total loss is the sum of the data loss and the regularization loss\n",
        "    loss=data_loss+regularization_loss\n",
        "\n",
        "    # -------- Accuracy Calculation --------\n",
        "    # Determine the predicted classes by taking the index of the maximum probability\n",
        "    pred=np.argmax(loss_act.output,axis=1)\n",
        "    # If y_train is one-hot encoded, convert it to class indices\n",
        "    if len(y_train.shape)==2:\n",
        "        y_train=np.argmax(y_train,axis=1)\n",
        "    # Calculating the accuracy as the proportion of correct predictions\n",
        "    acc=np.mean(pred==y_train)\n",
        "\n",
        "    if not i%100:\n",
        "        print(f'epoch: {i}, acc: {acc:.3f}, loss: {loss:.3f} (data_loss: {data_loss:.3f}, reg_loss: {regularization_loss:.3f}), lr: {opt.current_learning_rate} ')\n",
        "\n",
        "    # -------- Backward Pass --------\n",
        "    # Backpropagate through the loss layer to get the initial gradient\n",
        "    loss_act.backward(loss_act.output,y_train)\n",
        "    dense2.backward(loss_act.dinputs) # Backprop through second dense layer\n",
        "    drop1.backward(dense2.dinputs) # Backprop through dropout layer\n",
        "    act1.backward(drop1.dinputs) # Backprop through ReLU activation\n",
        "    dense1.backward(act1.dinputs) # Backprop through first dense layer\n",
        "\n",
        "    # -------- Parameter Update --------\n",
        "    opt.pre_update_param() # Adjust the learning rate if decay is used\n",
        "    opt.update_param(dense1) # Update parameters for the first dense layer\n",
        "    opt.update_param(dense2) # Update parameters for the second dense layer\n",
        "    opt.post_update_param() # Increment the iteration counter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFOpGq783s5Q",
        "outputId": "7417a370-fe73-4b6a-8b6a-fe0b514f63f2"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, acc: 0.666, loss: 0.645 (data_loss: 0.637, reg_loss: 0.008), lr: 0.02 \n",
            "epoch: 100, acc: 0.666, loss: 0.646 (data_loss: 0.637, reg_loss: 0.009), lr: 0.01998021958261321 \n",
            "epoch: 200, acc: 0.666, loss: 0.645 (data_loss: 0.637, reg_loss: 0.008), lr: 0.019960279044701046 \n",
            "epoch: 300, acc: 0.666, loss: 0.645 (data_loss: 0.637, reg_loss: 0.008), lr: 0.019940378268975763 \n",
            "epoch: 400, acc: 0.666, loss: 0.645 (data_loss: 0.637, reg_loss: 0.008), lr: 0.01992051713662487 \n",
            "epoch: 500, acc: 0.666, loss: 0.645 (data_loss: 0.637, reg_loss: 0.008), lr: 0.01990069552930875 \n",
            "epoch: 600, acc: 0.666, loss: 0.644 (data_loss: 0.637, reg_loss: 0.008), lr: 0.019880913329158343 \n",
            "epoch: 700, acc: 0.666, loss: 0.645 (data_loss: 0.637, reg_loss: 0.008), lr: 0.019861170418772778 \n",
            "epoch: 800, acc: 0.666, loss: 0.644 (data_loss: 0.637, reg_loss: 0.007), lr: 0.019841466681217078 \n",
            "epoch: 900, acc: 0.666, loss: 0.643 (data_loss: 0.637, reg_loss: 0.006), lr: 0.01982180200001982 \n",
            "epoch: 1000, acc: 0.666, loss: 0.643 (data_loss: 0.637, reg_loss: 0.006), lr: 0.019802176259170884 \n",
            "epoch: 1100, acc: 0.666, loss: 0.643 (data_loss: 0.637, reg_loss: 0.006), lr: 0.01978258934311912 \n",
            "epoch: 1200, acc: 0.666, loss: 0.643 (data_loss: 0.637, reg_loss: 0.006), lr: 0.01976304113677013 \n",
            "epoch: 1300, acc: 0.666, loss: 0.642 (data_loss: 0.637, reg_loss: 0.006), lr: 0.019743531525483964 \n",
            "epoch: 1400, acc: 0.666, loss: 0.642 (data_loss: 0.637, reg_loss: 0.005), lr: 0.01972406039507293 \n",
            "epoch: 1500, acc: 0.666, loss: 0.652 (data_loss: 0.645, reg_loss: 0.007), lr: 0.019704627631799327 \n",
            "epoch: 1600, acc: 0.666, loss: 0.643 (data_loss: 0.637, reg_loss: 0.006), lr: 0.019685233122373254 \n",
            "epoch: 1700, acc: 0.666, loss: 0.643 (data_loss: 0.637, reg_loss: 0.006), lr: 0.019665876753950384 \n",
            "epoch: 1800, acc: 0.666, loss: 0.642 (data_loss: 0.637, reg_loss: 0.005), lr: 0.01964655841412981 \n",
            "epoch: 1900, acc: 0.666, loss: 0.643 (data_loss: 0.637, reg_loss: 0.006), lr: 0.019627277990951823 \n",
            "epoch: 2000, acc: 0.666, loss: 0.643 (data_loss: 0.637, reg_loss: 0.006), lr: 0.019608035372895814 \n",
            "epoch: 2100, acc: 0.666, loss: 0.643 (data_loss: 0.637, reg_loss: 0.007), lr: 0.01958883044887805 \n",
            "epoch: 2200, acc: 0.666, loss: 0.643 (data_loss: 0.637, reg_loss: 0.006), lr: 0.019569663108249594 \n",
            "epoch: 2300, acc: 0.666, loss: 0.643 (data_loss: 0.637, reg_loss: 0.007), lr: 0.01955053324079414 \n",
            "epoch: 2400, acc: 0.666, loss: 0.643 (data_loss: 0.637, reg_loss: 0.006), lr: 0.019531440736725945 \n",
            "epoch: 2500, acc: 0.666, loss: 0.643 (data_loss: 0.637, reg_loss: 0.006), lr: 0.019512385486687673 \n",
            "epoch: 2600, acc: 0.666, loss: 0.643 (data_loss: 0.637, reg_loss: 0.006), lr: 0.019493367381748363 \n",
            "epoch: 2700, acc: 0.666, loss: 0.648 (data_loss: 0.641, reg_loss: 0.006), lr: 0.019474386313401298 \n",
            "epoch: 2800, acc: 0.666, loss: 0.644 (data_loss: 0.637, reg_loss: 0.008), lr: 0.019455442173562 \n",
            "epoch: 2900, acc: 0.666, loss: 0.644 (data_loss: 0.637, reg_loss: 0.007), lr: 0.019436534854566128 \n",
            "epoch: 3000, acc: 0.666, loss: 0.643 (data_loss: 0.637, reg_loss: 0.006), lr: 0.01941766424916747 \n",
            "epoch: 3100, acc: 0.666, loss: 0.643 (data_loss: 0.637, reg_loss: 0.006), lr: 0.019398830250535893 \n",
            "epoch: 3200, acc: 0.666, loss: 0.643 (data_loss: 0.637, reg_loss: 0.006), lr: 0.019380032752255354 \n",
            "epoch: 3300, acc: 0.666, loss: 0.642 (data_loss: 0.637, reg_loss: 0.006), lr: 0.01936127164832186 \n",
            "epoch: 3400, acc: 0.666, loss: 0.643 (data_loss: 0.637, reg_loss: 0.006), lr: 0.01934254683314152 \n",
            "epoch: 3500, acc: 0.666, loss: 0.643 (data_loss: 0.637, reg_loss: 0.006), lr: 0.019323858201528515 \n",
            "epoch: 3600, acc: 0.666, loss: 0.642 (data_loss: 0.637, reg_loss: 0.005), lr: 0.019305205648703173 \n",
            "epoch: 3700, acc: 0.666, loss: 0.642 (data_loss: 0.637, reg_loss: 0.005), lr: 0.01928658907028997 \n",
            "epoch: 3800, acc: 0.666, loss: 0.642 (data_loss: 0.637, reg_loss: 0.005), lr: 0.01926800836231563 \n",
            "epoch: 3900, acc: 0.666, loss: 0.642 (data_loss: 0.637, reg_loss: 0.005), lr: 0.019249463421207133 \n",
            "epoch: 4000, acc: 0.666, loss: 0.641 (data_loss: 0.637, reg_loss: 0.004), lr: 0.019230954143789846 \n",
            "epoch: 4100, acc: 0.666, loss: 0.641 (data_loss: 0.637, reg_loss: 0.004), lr: 0.019212480427285565 \n",
            "epoch: 4200, acc: 0.666, loss: 0.641 (data_loss: 0.637, reg_loss: 0.004), lr: 0.019194042169310647 \n",
            "epoch: 4300, acc: 0.666, loss: 0.641 (data_loss: 0.637, reg_loss: 0.004), lr: 0.019175639267874092 \n",
            "epoch: 4400, acc: 0.666, loss: 0.641 (data_loss: 0.637, reg_loss: 0.004), lr: 0.019157271621375684 \n",
            "epoch: 4500, acc: 0.666, loss: 0.640 (data_loss: 0.637, reg_loss: 0.003), lr: 0.0191389391286041 \n",
            "epoch: 4600, acc: 0.666, loss: 0.640 (data_loss: 0.637, reg_loss: 0.003), lr: 0.019120641688735073 \n",
            "epoch: 4700, acc: 0.666, loss: 0.641 (data_loss: 0.637, reg_loss: 0.004), lr: 0.019102379201329525 \n",
            "epoch: 4800, acc: 0.666, loss: 0.641 (data_loss: 0.637, reg_loss: 0.004), lr: 0.01908415156633174 \n",
            "epoch: 4900, acc: 0.666, loss: 0.640 (data_loss: 0.637, reg_loss: 0.003), lr: 0.01906595868406753 \n",
            "epoch: 5000, acc: 0.666, loss: 0.640 (data_loss: 0.637, reg_loss: 0.003), lr: 0.01904780045524243 \n",
            "epoch: 5100, acc: 0.666, loss: 0.640 (data_loss: 0.637, reg_loss: 0.003), lr: 0.019029676780939874 \n",
            "epoch: 5200, acc: 0.666, loss: 0.642 (data_loss: 0.637, reg_loss: 0.005), lr: 0.019011587562619416 \n",
            "epoch: 5300, acc: 0.666, loss: 0.641 (data_loss: 0.637, reg_loss: 0.004), lr: 0.01899353270211493 \n",
            "epoch: 5400, acc: 0.666, loss: 0.641 (data_loss: 0.637, reg_loss: 0.004), lr: 0.018975512101632844 \n",
            "epoch: 5500, acc: 0.666, loss: 0.640 (data_loss: 0.637, reg_loss: 0.004), lr: 0.018957525663750367 \n",
            "epoch: 5600, acc: 0.666, loss: 0.640 (data_loss: 0.637, reg_loss: 0.003), lr: 0.018939573291413745 \n",
            "epoch: 5700, acc: 0.666, loss: 0.640 (data_loss: 0.637, reg_loss: 0.003), lr: 0.018921654887936498 \n",
            "epoch: 5800, acc: 0.666, loss: 0.640 (data_loss: 0.637, reg_loss: 0.003), lr: 0.018903770356997706 \n",
            "epoch: 5900, acc: 0.666, loss: 0.639 (data_loss: 0.637, reg_loss: 0.003), lr: 0.018885919602640248 \n",
            "epoch: 6000, acc: 0.666, loss: 0.639 (data_loss: 0.637, reg_loss: 0.002), lr: 0.018868102529269144 \n",
            "epoch: 6100, acc: 0.666, loss: 0.639 (data_loss: 0.637, reg_loss: 0.002), lr: 0.018850319041649778 \n",
            "epoch: 6200, acc: 0.666, loss: 0.639 (data_loss: 0.637, reg_loss: 0.002), lr: 0.018832569044906263 \n",
            "epoch: 6300, acc: 0.666, loss: 0.639 (data_loss: 0.637, reg_loss: 0.002), lr: 0.018814852444519702 \n",
            "epoch: 6400, acc: 0.666, loss: 0.639 (data_loss: 0.637, reg_loss: 0.002), lr: 0.018797169146326564 \n",
            "epoch: 6500, acc: 0.666, loss: 0.638 (data_loss: 0.637, reg_loss: 0.002), lr: 0.01877951905651696 \n",
            "epoch: 6600, acc: 0.666, loss: 0.638 (data_loss: 0.637, reg_loss: 0.001), lr: 0.018761902081633034 \n",
            "epoch: 6700, acc: 0.666, loss: 0.638 (data_loss: 0.637, reg_loss: 0.001), lr: 0.018744318128567278 \n",
            "epoch: 6800, acc: 0.666, loss: 0.638 (data_loss: 0.637, reg_loss: 0.001), lr: 0.018726767104560903 \n",
            "epoch: 6900, acc: 0.666, loss: 0.638 (data_loss: 0.637, reg_loss: 0.001), lr: 0.018709248917202218 \n",
            "epoch: 7000, acc: 0.666, loss: 0.638 (data_loss: 0.637, reg_loss: 0.001), lr: 0.018691763474424996 \n",
            "epoch: 7100, acc: 0.666, loss: 0.638 (data_loss: 0.637, reg_loss: 0.001), lr: 0.018674310684506857 \n",
            "epoch: 7200, acc: 0.666, loss: 0.638 (data_loss: 0.637, reg_loss: 0.001), lr: 0.01865689045606769 \n",
            "epoch: 7300, acc: 0.666, loss: 0.638 (data_loss: 0.637, reg_loss: 0.001), lr: 0.01863950269806802 \n",
            "epoch: 7400, acc: 0.666, loss: 0.638 (data_loss: 0.637, reg_loss: 0.001), lr: 0.018622147319807447 \n",
            "epoch: 7500, acc: 0.666, loss: 0.638 (data_loss: 0.637, reg_loss: 0.001), lr: 0.018604824230923075 \n",
            "epoch: 7600, acc: 0.666, loss: 0.638 (data_loss: 0.637, reg_loss: 0.001), lr: 0.01858753334138793 \n",
            "epoch: 7700, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.001), lr: 0.018570274561509396 \n",
            "epoch: 7800, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.001), lr: 0.018553047801927663 \n",
            "epoch: 7900, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.018535852973614212 \n",
            "epoch: 8000, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.01851868998787026 \n",
            "epoch: 8100, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.018501558756325222 \n",
            "epoch: 8200, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.01848445919093522 \n",
            "epoch: 8300, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.018467391203981567 \n",
            "epoch: 8400, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.018450354708069265 \n",
            "epoch: 8500, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.018433349616125496 \n",
            "epoch: 8600, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.018416375841398172 \n",
            "epoch: 8700, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.01839943329745444 \n",
            "epoch: 8800, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.01838252189817921 \n",
            "epoch: 8900, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.018365641557773718 \n",
            "epoch: 9000, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.018348792190754044 \n",
            "epoch: 9100, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.0183319737119497 \n",
            "epoch: 9200, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.018315186036502167 \n",
            "epoch: 9300, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.018298429079863496 \n",
            "epoch: 9400, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.018281702757794862 \n",
            "epoch: 9500, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.018265006986365174 \n",
            "epoch: 9600, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.018248341681949654 \n",
            "epoch: 9700, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.018231706761228456 \n",
            "epoch: 9800, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.018215102141185255 \n",
            "epoch: 9900, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.018198527739105907 \n",
            "epoch: 10000, acc: 0.666, loss: 0.637 (data_loss: 0.637, reg_loss: 0.000), lr: 0.018181983472577025 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing\n",
        "# Forward pass on the test set\n",
        "dense1.forward(x_test)\n",
        "act1.forward(dense1.output)\n",
        "dense2.forward(act1.output)\n",
        "\n",
        "# Computing the loss on the test set\n",
        "loss=loss_act.forward(dense2.output,y_test)\n",
        "\n",
        "pred=np.argmax(loss_act.output,axis=1)\n",
        "if len(y_test.shape)==2:\n",
        "    y_test=np.argmax(y,axis=1)\n",
        "acc=np.mean(pred==y_test)\n",
        "\n",
        "print(f'Validation accuracy of RMSprop optimizer\\nacc: {acc:.3f}, loss: {loss:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NouXNvf345n",
        "outputId": "c581fe83-faef-4a44-b1fb-23105b2bbbb7"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy of RMSprop optimizer\n",
            "acc: 0.591, loss: 0.689\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Adam Optimizer"
      ],
      "metadata": {
        "id": "yQYGcH6K4Bmb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "opt=Adam(learning_rate=0.02,decay=1e-5)\n",
        "\n",
        "#Training\n",
        "for i in range(10001):\n",
        "    # -------- Forward Pass --------\n",
        "    dense1.forward(x_train) # First dense layer processing\n",
        "    act1.forward(dense1.output) # ReLU activation on dense1's output\n",
        "    drop1.forward(act1.output) # Dropout applied to the activated output\n",
        "    dense2.forward(drop1.output) # Second dense layer processing\n",
        "\n",
        "    # Computing the data loss using softmax and cross-entropy\n",
        "    data_loss=loss_act.forward(dense2.output,y_train)\n",
        "\n",
        "    # Computing the regularization loss from both dense layers\n",
        "    regularization_loss=(loss_act.loss.regularization_loss(dense1) + loss_act.loss.regularization_loss(dense2))\n",
        "\n",
        "    # Total loss is the sum of the data loss and the regularization loss\n",
        "    loss=data_loss+regularization_loss\n",
        "\n",
        "    # -------- Accuracy Calculation --------\n",
        "    # Determine the predicted classes by taking the index of the maximum probability\n",
        "    pred=np.argmax(loss_act.output,axis=1)\n",
        "    # If y_train is one-hot encoded, convert it to class indices\n",
        "    if len(y_train.shape)==2:\n",
        "        y_train=np.argmax(y_train,axis=1)\n",
        "    # Calculating the accuracy as the proportion of correct predictions\n",
        "    acc=np.mean(pred==y_train)\n",
        "\n",
        "    if not i%100:\n",
        "        print(f'epoch: {i}, acc: {acc:.3f}, loss: {loss:.3f} (data_loss: {data_loss:.3f}, reg_loss: {regularization_loss:.3f}), lr: {opt.current_learning_rate} ')\n",
        "\n",
        "    # -------- Backward Pass --------\n",
        "    # Backpropagate through the loss layer to get the initial gradient\n",
        "    loss_act.backward(loss_act.output,y_train)\n",
        "    dense2.backward(loss_act.dinputs) # Backprop through second dense layer\n",
        "    drop1.backward(dense2.dinputs) # Backprop through dropout layer\n",
        "    act1.backward(drop1.dinputs) # Backprop through ReLU activation\n",
        "    dense1.backward(act1.dinputs) # Backprop through first dense layer\n",
        "\n",
        "    # -------- Parameter Update --------\n",
        "    opt.pre_update_param() # Adjust the learning rate if decay is used\n",
        "    opt.update_param(dense1) # Update parameters for the first dense layer\n",
        "    opt.update_param(dense2) # Update parameters for the second dense layer\n",
        "    opt.post_update_param() # Increment the iteration counter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRXooRvO4A-m",
        "outputId": "d47c0953-e328-49d4-a02f-86b78aee56a4"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, acc: 0.666, loss: 0.680 (data_loss: 0.680, reg_loss: 0.000), lr: 0.02 \n",
            "epoch: 100, acc: 0.669, loss: 0.590 (data_loss: 0.588, reg_loss: 0.002), lr: 0.01998021958261321 \n",
            "epoch: 200, acc: 0.668, loss: 0.601 (data_loss: 0.598, reg_loss: 0.003), lr: 0.019960279044701046 \n",
            "epoch: 300, acc: 0.612, loss: 0.641 (data_loss: 0.636, reg_loss: 0.005), lr: 0.019940378268975763 \n",
            "epoch: 400, acc: 0.668, loss: 0.592 (data_loss: 0.586, reg_loss: 0.007), lr: 0.01992051713662487 \n",
            "epoch: 500, acc: 0.666, loss: 0.594 (data_loss: 0.586, reg_loss: 0.008), lr: 0.01990069552930875 \n",
            "epoch: 600, acc: 0.669, loss: 0.600 (data_loss: 0.590, reg_loss: 0.010), lr: 0.019880913329158343 \n",
            "epoch: 700, acc: 0.669, loss: 0.687 (data_loss: 0.675, reg_loss: 0.012), lr: 0.019861170418772778 \n",
            "epoch: 800, acc: 0.669, loss: 0.642 (data_loss: 0.633, reg_loss: 0.009), lr: 0.019841466681217078 \n",
            "epoch: 900, acc: 0.668, loss: 0.641 (data_loss: 0.635, reg_loss: 0.006), lr: 0.01982180200001982 \n",
            "epoch: 1000, acc: 0.668, loss: 0.640 (data_loss: 0.635, reg_loss: 0.005), lr: 0.019802176259170884 \n",
            "epoch: 1100, acc: 0.669, loss: 0.637 (data_loss: 0.633, reg_loss: 0.004), lr: 0.01978258934311912 \n",
            "epoch: 1200, acc: 0.669, loss: 0.636 (data_loss: 0.633, reg_loss: 0.003), lr: 0.01976304113677013 \n",
            "epoch: 1300, acc: 0.668, loss: 0.637 (data_loss: 0.635, reg_loss: 0.002), lr: 0.019743531525483964 \n",
            "epoch: 1400, acc: 0.668, loss: 0.640 (data_loss: 0.637, reg_loss: 0.004), lr: 0.01972406039507293 \n",
            "epoch: 1500, acc: 0.669, loss: 0.620 (data_loss: 0.614, reg_loss: 0.005), lr: 0.019704627631799327 \n",
            "epoch: 1600, acc: 0.669, loss: 0.602 (data_loss: 0.596, reg_loss: 0.006), lr: 0.019685233122373254 \n",
            "epoch: 1700, acc: 0.669, loss: 0.597 (data_loss: 0.590, reg_loss: 0.006), lr: 0.019665876753950384 \n",
            "epoch: 1800, acc: 0.669, loss: 0.596 (data_loss: 0.590, reg_loss: 0.006), lr: 0.01964655841412981 \n",
            "epoch: 1900, acc: 0.669, loss: 0.639 (data_loss: 0.633, reg_loss: 0.006), lr: 0.019627277990951823 \n",
            "epoch: 2000, acc: 0.669, loss: 0.636 (data_loss: 0.632, reg_loss: 0.004), lr: 0.019608035372895814 \n",
            "epoch: 2100, acc: 0.669, loss: 0.595 (data_loss: 0.589, reg_loss: 0.006), lr: 0.01958883044887805 \n",
            "epoch: 2200, acc: 0.666, loss: 0.605 (data_loss: 0.598, reg_loss: 0.007), lr: 0.019569663108249594 \n",
            "epoch: 2300, acc: 0.666, loss: 0.595 (data_loss: 0.586, reg_loss: 0.009), lr: 0.01955053324079414 \n",
            "epoch: 2400, acc: 0.666, loss: 0.644 (data_loss: 0.637, reg_loss: 0.007), lr: 0.019531440736725945 \n",
            "epoch: 2500, acc: 0.666, loss: 0.642 (data_loss: 0.637, reg_loss: 0.006), lr: 0.019512385486687673 \n",
            "epoch: 2600, acc: 0.666, loss: 0.641 (data_loss: 0.637, reg_loss: 0.005), lr: 0.019493367381748363 \n",
            "epoch: 2700, acc: 0.666, loss: 0.642 (data_loss: 0.637, reg_loss: 0.005), lr: 0.019474386313401298 \n",
            "epoch: 2800, acc: 0.666, loss: 0.642 (data_loss: 0.637, reg_loss: 0.005), lr: 0.019455442173562 \n",
            "epoch: 2900, acc: 0.666, loss: 0.641 (data_loss: 0.637, reg_loss: 0.004), lr: 0.019436534854566128 \n",
            "epoch: 3000, acc: 0.666, loss: 0.642 (data_loss: 0.637, reg_loss: 0.005), lr: 0.01941766424916747 \n",
            "epoch: 3100, acc: 0.666, loss: 0.642 (data_loss: 0.637, reg_loss: 0.005), lr: 0.019398830250535893 \n",
            "epoch: 3200, acc: 0.666, loss: 0.642 (data_loss: 0.637, reg_loss: 0.005), lr: 0.019380032752255354 \n",
            "epoch: 3300, acc: 0.666, loss: 0.642 (data_loss: 0.637, reg_loss: 0.006), lr: 0.01936127164832186 \n",
            "epoch: 3400, acc: 0.666, loss: 0.642 (data_loss: 0.637, reg_loss: 0.006), lr: 0.01934254683314152 \n",
            "epoch: 3500, acc: 0.666, loss: 0.642 (data_loss: 0.637, reg_loss: 0.005), lr: 0.019323858201528515 \n",
            "epoch: 3600, acc: 0.666, loss: 0.642 (data_loss: 0.637, reg_loss: 0.005), lr: 0.019305205648703173 \n",
            "epoch: 3700, acc: 0.666, loss: 0.642 (data_loss: 0.637, reg_loss: 0.005), lr: 0.01928658907028997 \n",
            "epoch: 3800, acc: 0.666, loss: 0.642 (data_loss: 0.637, reg_loss: 0.005), lr: 0.01926800836231563 \n",
            "epoch: 3900, acc: 0.666, loss: 0.644 (data_loss: 0.637, reg_loss: 0.008), lr: 0.019249463421207133 \n",
            "epoch: 4000, acc: 0.666, loss: 0.644 (data_loss: 0.637, reg_loss: 0.007), lr: 0.019230954143789846 \n",
            "epoch: 4100, acc: 0.666, loss: 0.643 (data_loss: 0.637, reg_loss: 0.007), lr: 0.019212480427285565 \n",
            "epoch: 4200, acc: 0.666, loss: 0.645 (data_loss: 0.637, reg_loss: 0.009), lr: 0.019194042169310647 \n",
            "epoch: 4300, acc: 0.666, loss: 0.645 (data_loss: 0.637, reg_loss: 0.008), lr: 0.019175639267874092 \n",
            "epoch: 4400, acc: 0.666, loss: 0.644 (data_loss: 0.637, reg_loss: 0.007), lr: 0.019157271621375684 \n",
            "epoch: 4500, acc: 0.666, loss: 0.644 (data_loss: 0.637, reg_loss: 0.007), lr: 0.0191389391286041 \n",
            "epoch: 4600, acc: 0.666, loss: 0.645 (data_loss: 0.637, reg_loss: 0.008), lr: 0.019120641688735073 \n",
            "epoch: 4700, acc: 0.666, loss: 0.644 (data_loss: 0.637, reg_loss: 0.007), lr: 0.019102379201329525 \n",
            "epoch: 4800, acc: 0.666, loss: 0.645 (data_loss: 0.637, reg_loss: 0.008), lr: 0.01908415156633174 \n",
            "epoch: 4900, acc: 0.666, loss: 0.644 (data_loss: 0.637, reg_loss: 0.007), lr: 0.01906595868406753 \n",
            "epoch: 5000, acc: 0.666, loss: 0.645 (data_loss: 0.637, reg_loss: 0.008), lr: 0.01904780045524243 \n",
            "epoch: 5100, acc: 0.666, loss: 0.644 (data_loss: 0.637, reg_loss: 0.007), lr: 0.019029676780939874 \n",
            "epoch: 5200, acc: 0.666, loss: 0.645 (data_loss: 0.637, reg_loss: 0.008), lr: 0.019011587562619416 \n",
            "epoch: 5300, acc: 0.666, loss: 0.644 (data_loss: 0.637, reg_loss: 0.007), lr: 0.01899353270211493 \n",
            "epoch: 5400, acc: 0.666, loss: 0.644 (data_loss: 0.637, reg_loss: 0.007), lr: 0.018975512101632844 \n",
            "epoch: 5500, acc: 0.666, loss: 0.643 (data_loss: 0.637, reg_loss: 0.006), lr: 0.018957525663750367 \n",
            "epoch: 5600, acc: 0.666, loss: 0.643 (data_loss: 0.637, reg_loss: 0.006), lr: 0.018939573291413745 \n",
            "epoch: 5700, acc: 0.666, loss: 0.642 (data_loss: 0.637, reg_loss: 0.006), lr: 0.018921654887936498 \n",
            "epoch: 5800, acc: 0.666, loss: 0.642 (data_loss: 0.637, reg_loss: 0.005), lr: 0.018903770356997706 \n",
            "epoch: 5900, acc: 0.669, loss: 0.639 (data_loss: 0.635, reg_loss: 0.005), lr: 0.018885919602640248 \n",
            "epoch: 6000, acc: 0.669, loss: 0.639 (data_loss: 0.634, reg_loss: 0.005), lr: 0.018868102529269144 \n",
            "epoch: 6100, acc: 0.669, loss: 0.639 (data_loss: 0.634, reg_loss: 0.006), lr: 0.018850319041649778 \n",
            "epoch: 6200, acc: 0.668, loss: 0.641 (data_loss: 0.635, reg_loss: 0.005), lr: 0.018832569044906263 \n",
            "epoch: 6300, acc: 0.668, loss: 0.640 (data_loss: 0.635, reg_loss: 0.005), lr: 0.018814852444519702 \n",
            "epoch: 6400, acc: 0.668, loss: 0.640 (data_loss: 0.636, reg_loss: 0.005), lr: 0.018797169146326564 \n",
            "epoch: 6500, acc: 0.668, loss: 0.641 (data_loss: 0.635, reg_loss: 0.005), lr: 0.01877951905651696 \n",
            "epoch: 6600, acc: 0.669, loss: 0.639 (data_loss: 0.634, reg_loss: 0.005), lr: 0.018761902081633034 \n",
            "epoch: 6700, acc: 0.669, loss: 0.638 (data_loss: 0.633, reg_loss: 0.005), lr: 0.018744318128567278 \n",
            "epoch: 6800, acc: 0.666, loss: 0.642 (data_loss: 0.637, reg_loss: 0.006), lr: 0.018726767104560903 \n",
            "epoch: 6900, acc: 0.666, loss: 0.642 (data_loss: 0.637, reg_loss: 0.005), lr: 0.018709248917202218 \n",
            "epoch: 7000, acc: 0.666, loss: 0.642 (data_loss: 0.637, reg_loss: 0.005), lr: 0.018691763474424996 \n",
            "epoch: 7100, acc: 0.666, loss: 0.641 (data_loss: 0.637, reg_loss: 0.005), lr: 0.018674310684506857 \n",
            "epoch: 7200, acc: 0.666, loss: 0.641 (data_loss: 0.637, reg_loss: 0.004), lr: 0.01865689045606769 \n",
            "epoch: 7300, acc: 0.666, loss: 0.641 (data_loss: 0.637, reg_loss: 0.004), lr: 0.01863950269806802 \n",
            "epoch: 7400, acc: 0.666, loss: 0.641 (data_loss: 0.637, reg_loss: 0.004), lr: 0.018622147319807447 \n",
            "epoch: 7500, acc: 0.666, loss: 0.641 (data_loss: 0.637, reg_loss: 0.004), lr: 0.018604824230923075 \n",
            "epoch: 7600, acc: 0.666, loss: 0.640 (data_loss: 0.637, reg_loss: 0.003), lr: 0.01858753334138793 \n",
            "epoch: 7700, acc: 0.666, loss: 0.640 (data_loss: 0.637, reg_loss: 0.003), lr: 0.018570274561509396 \n",
            "epoch: 7800, acc: 0.666, loss: 0.640 (data_loss: 0.637, reg_loss: 0.003), lr: 0.018553047801927663 \n",
            "epoch: 7900, acc: 0.666, loss: 0.642 (data_loss: 0.637, reg_loss: 0.005), lr: 0.018535852973614212 \n",
            "epoch: 8000, acc: 0.666, loss: 0.642 (data_loss: 0.637, reg_loss: 0.005), lr: 0.01851868998787026 \n",
            "epoch: 8100, acc: 0.666, loss: 0.642 (data_loss: 0.637, reg_loss: 0.005), lr: 0.018501558756325222 \n",
            "epoch: 8200, acc: 0.666, loss: 0.642 (data_loss: 0.637, reg_loss: 0.005), lr: 0.01848445919093522 \n",
            "epoch: 8300, acc: 0.666, loss: 0.643 (data_loss: 0.637, reg_loss: 0.005), lr: 0.018467391203981567 \n",
            "epoch: 8400, acc: 0.666, loss: 0.642 (data_loss: 0.637, reg_loss: 0.005), lr: 0.018450354708069265 \n",
            "epoch: 8500, acc: 0.666, loss: 0.643 (data_loss: 0.637, reg_loss: 0.006), lr: 0.018433349616125496 \n",
            "epoch: 8600, acc: 0.666, loss: 0.643 (data_loss: 0.637, reg_loss: 0.006), lr: 0.018416375841398172 \n",
            "epoch: 8700, acc: 0.666, loss: 0.642 (data_loss: 0.637, reg_loss: 0.005), lr: 0.01839943329745444 \n",
            "epoch: 8800, acc: 0.666, loss: 0.642 (data_loss: 0.637, reg_loss: 0.005), lr: 0.01838252189817921 \n",
            "epoch: 8900, acc: 0.666, loss: 0.643 (data_loss: 0.637, reg_loss: 0.006), lr: 0.018365641557773718 \n",
            "epoch: 9000, acc: 0.666, loss: 0.643 (data_loss: 0.637, reg_loss: 0.006), lr: 0.018348792190754044 \n",
            "epoch: 9100, acc: 0.666, loss: 0.644 (data_loss: 0.637, reg_loss: 0.007), lr: 0.0183319737119497 \n",
            "epoch: 9200, acc: 0.666, loss: 0.645 (data_loss: 0.637, reg_loss: 0.008), lr: 0.018315186036502167 \n",
            "epoch: 9300, acc: 0.666, loss: 0.646 (data_loss: 0.637, reg_loss: 0.009), lr: 0.018298429079863496 \n",
            "epoch: 9400, acc: 0.666, loss: 0.646 (data_loss: 0.637, reg_loss: 0.009), lr: 0.018281702757794862 \n",
            "epoch: 9500, acc: 0.666, loss: 0.646 (data_loss: 0.637, reg_loss: 0.009), lr: 0.018265006986365174 \n",
            "epoch: 9600, acc: 0.666, loss: 0.645 (data_loss: 0.637, reg_loss: 0.008), lr: 0.018248341681949654 \n",
            "epoch: 9700, acc: 0.666, loss: 0.644 (data_loss: 0.637, reg_loss: 0.007), lr: 0.018231706761228456 \n",
            "epoch: 9800, acc: 0.666, loss: 0.645 (data_loss: 0.637, reg_loss: 0.007), lr: 0.018215102141185255 \n",
            "epoch: 9900, acc: 0.666, loss: 0.644 (data_loss: 0.637, reg_loss: 0.007), lr: 0.018198527739105907 \n",
            "epoch: 10000, acc: 0.666, loss: 0.645 (data_loss: 0.637, reg_loss: 0.008), lr: 0.018181983472577025 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing\n",
        "# Forward pass on the test set\n",
        "dense1.forward(x_test)\n",
        "act1.forward(dense1.output)\n",
        "dense2.forward(act1.output)\n",
        "\n",
        "# Computing the loss on the test set\n",
        "loss=loss_act.forward(dense2.output,y_test)\n",
        "\n",
        "pred=np.argmax(loss_act.output,axis=1)\n",
        "if len(y_test.shape)==2:\n",
        "    y_test=np.argmax(y,axis=1)\n",
        "acc=np.mean(pred==y_test)\n",
        "\n",
        "print(f'Validation accuracy of Adam optimizer\\nacc: {acc:.3f}, loss: {loss:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5rM0yKj4NoU",
        "outputId": "fd4a0ec9-e018-40fd-9ba2-7f290904c088"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy of Adam optimizer\n",
            "acc: 0.591, loss: 0.689\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Based on the experimental results in this notebook, Adagrad demonstrated superior performance compared to the other optimizers. Specifically, Adagrad achieved a validation accuracy of approximately 62.3%, while SGD with momentum, RMSprop, and Adam each reached around 59.1% accuracy. This improvement suggests that Adagrad's adaptive learning rate, which adjusts individually for each parameter based on the historical accumulation of squared gradients, is particularly effective for this classification task. The ability to dynamically adapt the step size appears to help the model converge more efficiently on the diabetes dataset, making Adagrad a promising choice for similar optimization problems."
      ],
      "metadata": {
        "id": "qoWRcFd36w9n"
      }
    }
  ]
}